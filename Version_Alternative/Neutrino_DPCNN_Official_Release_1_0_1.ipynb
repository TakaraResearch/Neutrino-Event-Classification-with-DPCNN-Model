{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neutrino_DPCNN_Official_Release_1_0_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBJT4eioItJI",
        "colab_type": "text"
      },
      "source": [
        "#**Neutrino Event Classification with DPCNN Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md9jPDsPQKOB",
        "colab_type": "text"
      },
      "source": [
        "### **Intro: Neutrino Event Classification with DPCNN Model**\n",
        "\n",
        "Events of Background Alpha and Neutrino Beta\n",
        "from the *Jinping Simulation Setup* are classified in this notebook.\n",
        "\n",
        "The [Deep Pyramid Convolution Neural Network](https://www.aclweb.org/anthology/P17-1052.pdf) Method is implemented to generate results approximate to the dataset performance limit.\n",
        "\n",
        "The model has been optimized and simplified for further research and development. \n",
        "\n",
        "Tasks, Datasets and Results could be found [here](https://data-contest.applysquare.com/challenges/ghost-hunter-2020/dataset_files).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b4_w-P8aP5S",
        "colab_type": "text"
      },
      "source": [
        "### **Intro: Notebook Setups**\n",
        "The notebook is developed to run online with Google Colab. It also \n",
        "supports other notebook setups.\n",
        "\n",
        "The default work directory is '/content/drive/My Drive'.\n",
        "\n",
        "In the work directory,\n",
        "<br>Datasets should be found in './PhysicsData'.\n",
        "<br>Models will be saved in './Neutrino_DPCNN_Model/'.\n",
        "<br>Answers will be produced in './DPCNN_Answers/'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQroyCLChbIK",
        "colab_type": "text"
      },
      "source": [
        "###**System Basics**\n",
        "Mount Google Drive. \n",
        "\n",
        "Check Cuda, GPU, Python and Pytorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxeHQ09O6gh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1SEzrFdN6kG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvcc -V"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FkiMElh6ZwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Basics Setup\n",
        "import sys\n",
        "print('python version:',sys.version)\n",
        "import torch\n",
        "print('torch version:',torch.__version__,end=' | ')\n",
        "print('cuda available:',torch.cuda.is_available(),end=' | ')\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print('device count:',torch.cuda.device_count())\n",
        "  print('current device:',torch.cuda.current_device(),end=' | ')\n",
        "  print('device name:',torch.cuda.get_device_name(0),end=' | ') # default=current\n",
        "  print('cuda capability:',torch.cuda.get_device_capability(0)) # default=current "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ySsvsu1iCxI",
        "colab_type": "text"
      },
      "source": [
        "###**Basic functions for Dataset Processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldPsk_TGeodq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7sjQEwA56DZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import os\n",
        "# os.chdir('/content/drive/My Drive')\n",
        "\n",
        "import tables\n",
        "import numpy as np\n",
        "\n",
        "import math\n",
        "\n",
        "from torch import nn\n",
        "import torch\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.utils.data as Data\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "\n",
        "import os\n",
        "\n",
        "## Basic Functions for Data Processing\n",
        "\n",
        "# Wave Channel\n",
        "def make_wave(wave_form,denoise='True'):\n",
        "    shift = np.argmax(np.bincount(wave_form))  # make baseline shifts, normally 972\n",
        "    shift_wave = -wave_form + shift  # non_negative_peak + zero_base_level\n",
        "    cut_wave = shift_wave[5:]\n",
        "    if denoise != 'False':\n",
        "      cut_wave[cut_wave<5]=0\n",
        "    return cut_wave\n",
        "def make_random_wave(length):\n",
        "    rand_wave = np.round(np.random.randn(length) / 1.5).tolist()\n",
        "    return rand_wave\n",
        "\n",
        "# Event Type\n",
        "def make_type_vec(type):\n",
        "    type_vec = [0.5,0.5]\n",
        "    if type == 0:\n",
        "        type_vec = [0,1]\n",
        "    elif type == 1:\n",
        "        type_vec = [1,0]\n",
        "    else:\n",
        "        print('Type Error')\n",
        "    return type_vec\n",
        "\n",
        "# Dataset Functions\n",
        "def make_dataset(WaveTable, TypeTable, StartWaveIndex, StartEventIndex, Period):\n",
        "    Wave_Index = StartWaveIndex\n",
        "    Wave_Index_Num = len(WaveTable)\n",
        "    Event_Index_Num = len(TypeTable)\n",
        "    Wave_Data = []\n",
        "    Type_Data = []\n",
        "    for Particle_Index in range(StartEventIndex, min(StartEventIndex + Period, Event_Index_Num)):\n",
        "        # Read from TypeTable\n",
        "        EventID_Particle = TypeTable[Particle_Index]['EventID']\n",
        "        Type_Particle = TypeTable[Particle_Index]['Alpha']\n",
        "\n",
        "        # Read WaveTable and make Event Wave\n",
        "        Event_Wave = []\n",
        "        for Channel_Index in range(30):\n",
        "            if Wave_Index < Wave_Index_Num: #Debugged for the Final Wave\n",
        "                EventID_Waveform = WaveTable[Wave_Index]['EventID']\n",
        "                ChannelID_Waveform = WaveTable[Wave_Index]['ChannelID']\n",
        "                Waveform_Waveform = WaveTable[Wave_Index]['Waveform']\n",
        "                # Process with Abnormal_Check\n",
        "                if EventID_Waveform < EventID_Particle:\n",
        "                    print('Abnormal 1, EventScan passes WaveEvent')\n",
        "                elif EventID_Waveform == EventID_Particle:\n",
        "                    if ChannelID_Waveform < Channel_Index:\n",
        "                        print('Abnormal 2, ChannelScan passes WaveChannel')\n",
        "                    elif ChannelID_Waveform == Channel_Index:\n",
        "                        Chan_Wave = make_wave(Waveform_Waveform)\n",
        "                        Wave_Index += 1\n",
        "                        Event_Wave.append(Chan_Wave)\n",
        "                    elif ChannelID_Waveform > Channel_Index:\n",
        "                        Chan_Wave = make_random_wave(1024)\n",
        "                        Event_Wave.append(Chan_Wave)\n",
        "                    else:\n",
        "                        print('Error: Channel Logic Error')\n",
        "                elif EventID_Waveform > EventID_Particle:\n",
        "                    Chan_Wave = make_random_wave(1024)\n",
        "                    Event_Wave.append(Chan_Wave)\n",
        "                else:\n",
        "                    print('Error: Event Logic Error')\n",
        "                if len(Chan_Wave) != 1024:\n",
        "                    print(\"length not match\")\n",
        "            else:\n",
        "              Chan_Wave = make_random_wave(1024)\n",
        "              Event_Wave.append(Chan_Wave)\n",
        "        \n",
        "        # Read Type\n",
        "        Event_Type = make_type_vec(Type_Particle)\n",
        "\n",
        "        # Write to Data_Base\n",
        "        Wave_Data.append(Event_Wave)\n",
        "        Type_Data.append(Event_Type)\n",
        "\n",
        "    Wave_Data_NP = np.array(Wave_Data)\n",
        "    Type_Data_NP = np.array(Type_Data)\n",
        "\n",
        "    EndWaveIndex = Wave_Index - 1\n",
        "    EndEventIndex = min(StartEventIndex + Period - 1, Event_Index_Num)\n",
        "    return Wave_Data_NP, Type_Data_NP, EndWaveIndex, EndEventIndex\n",
        "\n",
        "print(\"Basics Functions Defined\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtPUcJUzqNa5",
        "colab_type": "text"
      },
      "source": [
        "### **DPCNN Structure Defination**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56CmR_zbdvO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Deep Pyramid Convolutional Neural Networks for Text Categorization'''\n",
        "# Pre-Activation Setup\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DPCNN(nn.Module):\n",
        "    def __init__(self, num_filters=250):\n",
        "        super(DPCNN, self).__init__()\n",
        "        self.conv_region_embedding = nn.Conv1d(30, num_filters, kernel_size=7, padding=3)\n",
        "        #Kernel_size 9 is also encouraged, padding should be 4 in these cases, 3/1 is also feasible\n",
        "        self.conv3 = nn.Conv1d(num_filters, num_filters, kernel_size=3, padding=1)\n",
        "        self.max_pool = nn.MaxPool1d(kernel_size=3, stride=2)\n",
        "        self.act_fun = nn.ReLU(True)\n",
        "        self.bn = nn.BatchNorm1d(num_filters)\n",
        "        self.fc = nn.Linear(num_filters,2)\n",
        "        self.channel_size = num_filters\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def _block(self,x):\n",
        "\n",
        "        # Pooling\n",
        "        px = self.max_pool(x)\n",
        "\n",
        "        # Convolution\n",
        "        x = self.bn(px)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv3(x)\n",
        "\n",
        "        x = self.bn(px)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv3(x)\n",
        "\n",
        "        # Short Cut\n",
        "        x = x + px\n",
        "        return x\n",
        "    \n",
        "    def forward(self,x):\n",
        "        batch = x.shape[0]\n",
        "        \n",
        "        # Region embedding\n",
        "        x = self.conv_region_embedding(x)   \n",
        "        \n",
        "        # [batch_size, channel_size, length]\n",
        "        x = self.act_fun(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.act_fun(x)\n",
        "        x = self.conv3(x)\n",
        "\n",
        "        while x.size()[2] > 2:\n",
        "            x = self._block(x)\n",
        "                \n",
        "        x = self.bn(x)\n",
        "        x = x.view(batch, self.channel_size) #squeeze\n",
        "        x = self.fc(x)\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "print(\"DPCNN Structure Defined\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxt6HrOpsMAU",
        "colab_type": "text"
      },
      "source": [
        "### **Neural Network Initialization**\n",
        "Start from zero, or reload a pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfu4tzdgd6G0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Start from zero\n",
        "net = DPCNN()\n",
        "if torch.cuda.is_available():\n",
        "    net=net.cuda()\n",
        "optimizer = optim.Adam(net.parameters(), lr=1e-3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKRJ_4NytqOJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load a pre-trained model\n",
        "#net = torch.load(\"./Neutrino_DPCNN_Model/Model_Name\") #Change the Model_Name when needed \n",
        "net = torch.load(\"./Neutrino_DPCNN_Model/Loss_0.4489_R0_pre-0\")\n",
        "if torch.cuda.is_available():\n",
        "    net=net.cuda()\n",
        "optimizer = optim.Adam(net.parameters(), lr=5e-6)\n",
        "#optimizer = optim.SGD(net.parameters(), lr=1e-5, momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2VgzOEOv7Cn",
        "colab_type": "text"
      },
      "source": [
        "###**Network Training**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZkU3_UceAjj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Basic Factors\n",
        "Division_Period = 5000\n",
        "BATCHSIZE = 100\n",
        "EPOCH= 1\n",
        "ROUND= 20\n",
        "\n",
        "LoadPath = './PhysicsData/'\n",
        "FileList = ['pre-0','pre-1','pre-2','pre-3','pre-4']\n",
        "for round in range(ROUND):\n",
        "    for FileName in FileList:\n",
        "        fullfilename = LoadPath + FileName + \".h5\"\n",
        "        # Read hdf5 file\n",
        "        h5file = tables.open_file(fullfilename, \"r\")\n",
        "        WaveformTable = h5file.root.Waveform\n",
        "        ParticleTruthTable = h5file.root.ParticleTruth\n",
        "        # Statistics\n",
        "        print(\"File\", FileName)\n",
        "        print(len(WaveformTable), \"Wave entries\")\n",
        "        print(len(ParticleTruthTable), \"Particle entries\")\n",
        "\n",
        "        Wave_Start_Point = 0\n",
        "        Event_Start_Point = 0\n",
        "        Training_Division_Num = math.ceil((len(ParticleTruthTable) - Event_Start_Point) / Division_Period)\n",
        "        # Loop through division parts\n",
        "        for Division_Index in range(Training_Division_Num):\n",
        "            #Make Dataset\n",
        "            Wave_DataSet, Type_DataSet, Wave_End_Point, Event_End_Point = \\\n",
        "                make_dataset(WaveformTable, ParticleTruthTable, Wave_Start_Point, Event_Start_Point, Division_Period)\n",
        "            print(\"File\", FileName, end=\", \")\n",
        "            print(\"Event\", Event_Start_Point, '-', Event_End_Point, \"Created\", end=\", \")\n",
        "            print(\"Wave\", Wave_Start_Point, '-', Wave_End_Point, \"Processed\")\n",
        "\n",
        "            #Multiple Epochs are supported to increase training efficiency\n",
        "            for epoch in range(EPOCH):\n",
        "                #Datasets\n",
        "                Wave_train, Wave_test, Type_train, Type_test = train_test_split(Wave_DataSet, Type_DataSet, test_size=0.2, random_state=42)\n",
        "                Wave_train_torch = torch.from_numpy(Wave_train).float()\n",
        "                Type_train_torch = torch.from_numpy(Type_train).float()\n",
        "                Wave_test_torch = torch.from_numpy(Wave_test).float()\n",
        "                Type_test_torch = torch.from_numpy(Type_test).float()\n",
        "                if torch.cuda.is_available():\n",
        "                    Wave_train_torch=Wave_train_torch.cuda()\n",
        "                    Type_train_torch=Type_train_torch.cuda()\n",
        "                    Wave_test_torch=Wave_test_torch.cuda()\n",
        "                    Type_test_torch=Type_test_torch.cuda()\n",
        "                train_data = Data.TensorDataset(Wave_train_torch,Type_train_torch)\n",
        "                train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCHSIZE, shuffle=True)\n",
        "                test_data = Data.TensorDataset(Wave_test_torch, Type_test_torch)\n",
        "                test_loader = Data.DataLoader(dataset=test_data, batch_size=BATCHSIZE, shuffle=False)\n",
        "\n",
        "                #Training\n",
        "                running_loss = 0.0\n",
        "                train_count = 0\n",
        "                for i, data in enumerate(train_loader, 0):\n",
        "                    # get the inputs\n",
        "                    inputs, labels = data\n",
        "                    inputs, labels = Variable(inputs), Variable(labels)\n",
        "                    # zero the parameter gradients\n",
        "                    optimizer.zero_grad()\n",
        "                    # forward + backward + optimize\n",
        "                    outputs = net(inputs)\n",
        "                    criterion = nn.BCELoss()\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    running_loss += loss.data.item()\n",
        "                    train_count+=1\n",
        "                train_loss = running_loss/(train_count)\n",
        "                print('Training loss: %.5f'%(train_loss),end=\", \")\n",
        "\n",
        "                #Checking\n",
        "                cum_loss = 0.0\n",
        "                test_count = 0\n",
        "                for i, data in enumerate(test_loader, 0):\n",
        "                    # get the inputs\n",
        "                    inputs, labels = data\n",
        "                    inputs, labels = Variable(inputs), Variable(labels)\n",
        "                    # forward only\n",
        "                    outputs = net(inputs)\n",
        "                    criterion = nn.BCELoss()\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    cum_loss += loss.data.item()\n",
        "                    test_count += 1\n",
        "                test_loss = cum_loss/(test_count)\n",
        "                print('Testing loss: %.5f'%(test_loss))\n",
        "\n",
        "            #Division Factor Update\n",
        "            Wave_Start_Point = Wave_End_Point + 1\n",
        "            Event_Start_Point = Event_End_Point + 1\n",
        "\n",
        "        print(\"h5 file processing finished\")\n",
        "        h5file.close()\n",
        "\n",
        "        #Save Network Model\n",
        "        SavePath = './Neutrino_DPCNN_Model/'\n",
        "        if not os.path.exists(SavePath):\n",
        "            os.makedirs(SavePath)\n",
        "        save_name = SavePath+\"Loss_\"+\"%.4f\"%(test_loss)+\"_R\"+str(round)+\"_\"+FileName\n",
        "        torch.save(net,save_name)\n",
        "\n",
        "print(\"Training Finished\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMB-5R5u2k5n",
        "colab_type": "text"
      },
      "source": [
        "###**Generating Answers**\n",
        "Run Directly from trained network, or load from a pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fuSGB-DwRpsA",
        "colab": {}
      },
      "source": [
        "# Load a pre-trained model\n",
        "#net = torch.load(\"./Neutrino_DPCNN_Model/Model_Name\") #Change the Model_Name when needed\n",
        "net = torch.load(\"./Neutrino_DPCNN_Model/Loss_0.4489_R0_pre-0\")\n",
        "if torch.cuda.is_available():\n",
        "    net=net.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyoQg6Hd2i0z",
        "colab_type": "text"
      },
      "source": [
        "Answer Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbJvCSR22pYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import os\n",
        "# os.chdir('/content/drive/My Drive')\n",
        "\n",
        "import tables\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from torch import nn\n",
        "import torch\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.utils.data as Data\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "\n",
        "import os\n",
        "\n",
        "## Basics Processing Functions\n",
        "# Wave Channel\n",
        "def make_wave(wave_form,denoise='True'):\n",
        "  shift = np.argmax(np.bincount(wave_form))  # make baseline shifts, normally 972\n",
        "  shift_wave = -wave_form + shift  # non_negative_peak + zero_base_level\n",
        "  cut_wave = shift_wave[5:]\n",
        "  if denoise != 'False':\n",
        "    cut_wave[cut_wave<5]=0\n",
        "  return cut_wave\n",
        "def make_random_wave(length):\n",
        "  rand_wave = np.round(np.random.randn(length) / 1.5).tolist()\n",
        "  return rand_wave\n",
        "\n",
        "## Answer Functions\n",
        "# Calculate Event Num and make ID Count\n",
        "def make_event_count_and_list(WaveTable):\n",
        "  ChanNum = len(WaveTable)\n",
        "  ID_count=0\n",
        "  ID=[]\n",
        "  last_ID = -10\n",
        "  #Search Trough Channel\n",
        "  for WaveIndex in range(ChanNum):\n",
        "    eventID = WaveformTable[WaveIndex]['EventID']\n",
        "    # write in for every new event\n",
        "    if eventID != last_ID:\n",
        "      ID_count+=1\n",
        "      last_ID = eventID\n",
        "      ID.append(last_ID)\n",
        "  return ID, ID_count\n",
        "\n",
        "# Dataset Functions\n",
        "def make_predict_set(WaveTable, Event_Num, StartWaveIndex, StartEventIndex, Period):\n",
        "  Wave_Index = StartWaveIndex\n",
        "  Event_Index_Num = Event_Num\n",
        "  Wave_Index_Num = len(WaveTable)\n",
        "  Wave_Data = []\n",
        "  #Pos_Vec=get_sinusoid_encoding_vec(1024,20)\n",
        "  for Particle_Index in range(StartEventIndex, min(StartEventIndex + Period, Event_Index_Num+1)):\n",
        "    # Take Particle_Index\n",
        "    EventID_Particle = Particle_Index  \n",
        "    # Read WaveTable and make Event Wave\n",
        "    Event_Wave = []\n",
        "    for Channel_Index in range(30):\n",
        "      if Wave_Index < Wave_Index_Num: #Debugged for the Final Wave\n",
        "        EventID_Waveform = WaveTable[Wave_Index]['EventID']\n",
        "        ChannelID_Waveform = WaveTable[Wave_Index]['ChannelID']\n",
        "        Waveform_Waveform = WaveTable[Wave_Index]['Waveform']\n",
        "        # Process with Abnormal_Check\n",
        "        if EventID_Waveform < EventID_Particle:\n",
        "          print('Abnormal 1, EventScan passes WaveEvent')\n",
        "          print(EventID_Waveform,EventID_Particle)\n",
        "          raise SyntaxError('Error: EventScan passes WaveEvent')\n",
        "        elif EventID_Waveform == EventID_Particle:\n",
        "          if ChannelID_Waveform < Channel_Index:\n",
        "            print('Abnormal 2, ChannelScan passes WaveChannel')\n",
        "          elif ChannelID_Waveform == Channel_Index:\n",
        "            Chan_Wave = make_wave(Waveform_Waveform)\n",
        "            Wave_Index += 1\n",
        "            Event_Wave.append(Chan_Wave)\n",
        "          elif ChannelID_Waveform > Channel_Index:\n",
        "            Chan_Wave = make_random_wave(1024)\n",
        "            Event_Wave.append(Chan_Wave)\n",
        "          else:\n",
        "            print('Error: Channel Logic Error')\n",
        "        elif EventID_Waveform > EventID_Particle:\n",
        "          Chan_Wave = make_random_wave(1024)\n",
        "          Event_Wave.append(Chan_Wave)\n",
        "        else:\n",
        "          print('Error: Event Logic Error')\n",
        "        if len(Chan_Wave) != 1024:\n",
        "          print(\"length not match\")\n",
        "      else:\n",
        "        Chan_Wave = make_random_wave(1024)\n",
        "        Event_Wave.append(Chan_Wave)\n",
        "\n",
        "    #Event_Wave.extend(Pos_Vec)\n",
        "    # Write to Data_Base\n",
        "    Wave_Data.append(Event_Wave)\n",
        "\n",
        "  Wave_Data_NP = np.array(Wave_Data)\n",
        "  EndWaveIndex = Wave_Index - 1\n",
        "  EndEventIndex = min(StartEventIndex + Period - 1, Event_Index_Num)\n",
        "  return Wave_Data_NP, EndWaveIndex, EndEventIndex\n",
        "  \n",
        "################\n",
        "#net()\n",
        "\n",
        "## Basic Factors\n",
        "Division_Period = 4000\n",
        "BATCHSIZE = 64\n",
        "\n",
        "LoadPath = \"./PhysicsData/\"\n",
        "FileName= \"pre-problem\"\n",
        "fullfilename = LoadPath + FileName + \".h5\"\n",
        "# Read hdf5 file\n",
        "h5file = tables.open_file(fullfilename, \"r\")\n",
        "WaveformTable = h5file.root.Waveform\n",
        "EventList,ID_Num = make_event_count_and_list(WaveformTable) #0 not exist\n",
        "EventNum = WaveformTable[-1]['EventID']-WaveformTable[0]['EventID']+1\n",
        "\n",
        "# Statistics\n",
        "print(\"File\", FileName)\n",
        "print(len(WaveformTable), \"Wave Entries\")\n",
        "print(EventNum, \"Particle Entries\")\n",
        "print(ID_Num, \"Real Entries\")\n",
        "\n",
        "Wave_Start_Point = 0\n",
        "Event_Start_Point = 1\n",
        "Predicting_Division_Num = math.ceil((EventNum - Event_Start_Point) / Division_Period)\n",
        "\n",
        "# Loop through division parts\n",
        "Output_Data = []\n",
        "for Division_Index in range(Predicting_Division_Num):\n",
        "  #Make Dataset\n",
        "  Wave_DataSet, Wave_End_Point, Event_End_Point = \\\n",
        "      make_predict_set(WaveformTable, EventNum, Wave_Start_Point, Event_Start_Point, Division_Period)\n",
        "  print(\"Event\", Event_Start_Point, '-', Event_End_Point, \"Created\", end=\", \")\n",
        "  print(\"Wave\", Wave_Start_Point, '-', Wave_End_Point, \"Processed\")\n",
        "\n",
        "  # Making Dataset\n",
        "  predict_data = torch.from_numpy(Wave_DataSet).float()\n",
        "  if torch.cuda.is_available():\n",
        "    predict_data=predict_data.cuda()\n",
        "  predict_loader = Data.DataLoader(dataset=predict_data, batch_size=BATCHSIZE, shuffle=False)\n",
        "\n",
        "  # Makeing Output\n",
        "  for i, data in enumerate(predict_loader, 0):\n",
        "    inputs = Variable(data)\n",
        "    outputs = net(inputs)\n",
        "    if torch.cuda.is_available():\n",
        "      batch_output = outputs.cpu().data.numpy()\n",
        "    else:\n",
        "      batch_output = outputs.data.numpy()\n",
        "    batch_predict = batch_output\n",
        "    Output_Data.extend(batch_predict)\n",
        "\n",
        "  #Division Factor Update\n",
        "  Wave_Start_Point = Wave_End_Point + 1\n",
        "  Event_Start_Point = Event_End_Point + 1\n",
        "\n",
        "OutputData = np.array(Output_Data)\n",
        "#Value check inplemented in case of needed\n",
        "OutputData[OutputData<0]=0\n",
        "OutputData[OutputData>1]=1\n",
        "print(\"Answer \", len(OutputData),\" Channels Generated\")\n",
        "\n",
        "################################################\n",
        "\n",
        "# Make Output Directory\n",
        "SavePath = \"./DPCNN_Answers/\"\n",
        "if not os.path.exists(SavePath):\n",
        "    os.makedirs(SavePath)\n",
        "\n",
        "# Writing a file.\n",
        "# Define the database columns\n",
        "class AnswerData(tables.IsDescription):\n",
        "    EventID = tables.Int64Col(pos=0)\n",
        "    Alpha = tables.Float32Col(pos=1)\n",
        "\n",
        "# Create the output file and the group\n",
        "h5file_write = tables.open_file(SavePath + \"DPCNN_Neutrino_Predict.h5\", mode=\"w\", title=\"OneTonDetector\")\n",
        "# Create tables\n",
        "AnswerTable = h5file_write.create_table(\"/\", \"Answer\", AnswerData, \"Answer\")\n",
        "answer = AnswerTable.row\n",
        "\n",
        "for j in EventList:\n",
        "    answer['EventID'] = j\n",
        "    answer['Alpha'] = OutputData[j-1][0]\n",
        "    answer.append()\n",
        "# Flush into the output file\n",
        "\n",
        "AnswerTable.flush()\n",
        "print(len(AnswerTable), \" Events\")\n",
        "\n",
        "h5file_write.close()\n",
        "h5file.close()\n",
        "\n",
        "print(\"Answer Produced\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmnYe3S8xMgp",
        "colab_type": "text"
      },
      "source": [
        "###**Other Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyQolWDbh9oi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check void wave record event(s)\n",
        "jump=0\n",
        "for i in range(len(EventList)):\n",
        "  if EventList[i] != i+jump:\n",
        "    print(\"event do not exist\",i+jump)\n",
        "    jump+=1\n",
        "\n",
        "print(jump,' non-input event(s)')\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}